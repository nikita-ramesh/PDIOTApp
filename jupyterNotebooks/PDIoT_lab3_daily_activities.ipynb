{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzd9Z9OZxNvd"
   },
   "source": [
    "# **Lab 3 - Activity Recognition with Machine Learning**\n",
    "\n",
    "This notebook implements a machine learning workflow to recognize different physical activities from Respeck sensor data. The dataset includes multiple 30-second recordings of various physical activities (e.g., ascending stairs, shuffle walking, sitting-standing) stored in separate CSV files for each activity.\n",
    "\n",
    "You will then use the model you develop here and deploy it inside your Android app for live classification.\n",
    "\n",
    "In this week, you will not have access to the full dataset as of yet. However, you can complete this lab by combining the data that you and your group mates have collected in Coursework 1 as proof-of-concept first for when you eventually receive the full dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXyHZD1A0X7J"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "id": "s2B8Hymdj1Sg"
   },
   "outputs": [],
   "source": [
    "# Importing libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Input, concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icbrBf1Kl6vp"
   },
   "source": [
    "# Reading Files\n",
    "Reading files from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "id": "pTsJd33Kl44J"
   },
   "outputs": [],
   "source": [
    "# # Put in the path of your dataset here\n",
    "# respeck_dataset_path = \"C:/Users/nikit/Documents/University/Year4/PDIOT/CW3/dataset/Respeck/DailyActivities/\"\n",
    "# thingy_dataset_path = \"C:/Users/nikit/Documents/University/Year4/PDIOT/CW3/dataset/Thingy/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOTe3o9Il4ST"
   },
   "source": [
    "This line uses the glob module to find all file paths that match a specified pattern. The 'glob.glob()' function returns a list of file paths that match the given pattern. `your_dataset_path` should be the directory where your dataset files are located.\n",
    "\n",
    "The `*` is a wildcard character that matches any string of characters,  so this pattern retrieves all folders in the 'your_dataset_path' directory.\n",
    "\n",
    "Below is just an example of what your dataset folder can look like. You should refer to the Coursework 3 instructions on what classes your model(s) are expected to be able to classify. Within your dataset directory, there should be subfolders, each representing a class of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4izGxKkllz6",
    "outputId": "49f7031e-36ae-454a-a8dc-4a7e92243315"
   },
   "outputs": [],
   "source": [
    "# glob.glob(thingy_dataset_path + \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(respeck_dataset_path + \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3zJiRjym7LU"
   },
   "source": [
    "To see the files in each subfolder you can similarly do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCS4lQgwnO9X",
    "outputId": "14dd9919-74f0-462f-e9a3-3122040278ce"
   },
   "outputs": [],
   "source": [
    "# activity_folder = \"\"\n",
    "# glob.glob(respeck_dataset_path + \"/\"+activity_folder+\"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_folder = \"\"\n",
    "# glob.glob(thingy_dataset_path + \"/\"+activity_folder+\"/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7eNuiHKmBuT"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zdg12YooOJF"
   },
   "source": [
    "## Load list of files in an activity folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "id": "b_ZtuAb64ZsD"
   },
   "outputs": [],
   "source": [
    "# def load_files_from_folder(folder_path):\n",
    "#     \"\"\"\n",
    "#     Load all CSV files from a folder and return a list of file paths.\n",
    "\n",
    "#     Parameters:\n",
    "#     folder_path (str): The path to the folder containing CSV files.\n",
    "\n",
    "#     Returns:\n",
    "#     list: A list of file paths for all CSV files in the folder.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Initialize an empty list to store the full file paths of the CSV files\n",
    "#     file_paths = []\n",
    "\n",
    "#     # Loop through all the files in the given folder\n",
    "#     for file_name in os.listdir(folder_path):\n",
    "#         # Check if the file has a .csv extension (ignores other files)\n",
    "#         if file_name.endswith('.csv'):\n",
    "#             # Construct the full file path by joining the folder path and the file name\n",
    "#             full_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "#             # Append the full file path to the file_paths list\n",
    "#             file_paths.append(full_file_path)\n",
    "\n",
    "#     # Return the complete list of CSV file paths\n",
    "#     return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_files_from_multiple_folders(base_path, subfolders):\n",
    "    \"\"\"\n",
    "    Load all CSV files from multiple folders and return a combined list of file paths.\n",
    "    \n",
    "    Parameters:\n",
    "    base_path (str): The base path containing year folders.\n",
    "    subfolders (list): List of subfolder paths relative to base_path for each dataset.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of file paths for all CSV files in the specified folders.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        folder_path = os.path.join(base_path, subfolder)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_paths.append(os.path.join(folder_path, file_name))\n",
    "                \n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path\n",
    "base_path = \"C:/Users/nikit/Documents/University/Year4/PDIOT/CW3/pdiotapp/datasets\"\n",
    "\n",
    "# Subfolders for each dataset type\n",
    "respeck_subfolders = [\"year1/Respeck/DailyActivities\", \"year2/Respeck/DailyActivities\"]\n",
    "thingy_subfolders = [\"year1/Thingy\", \"year2/Thingy\"]\n",
    "\n",
    "# Prepare paths to be used by `process_activity`\n",
    "respeck_paths = [os.path.join(base_path, subfolder) for subfolder in respeck_subfolders]\n",
    "thingy_paths = [os.path.join(base_path, subfolder) for subfolder in thingy_subfolders]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUGBeBBn_L8"
   },
   "source": [
    "## Train and test set split from list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "id": "2SzHoQz2NH3v"
   },
   "outputs": [],
   "source": [
    "def split_files(file_list, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split the list of files into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    file_list (list): List of file paths to be split into train and test sets.\n",
    "    test_size (float): The proportion of files to allocate to the test set.\n",
    "                       Default is 0.2, meaning 20% of the files will be used for testing.\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - train_files (list): List of file paths for the training set.\n",
    "        - test_files (list): List of file paths for the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the file list into training and test sets using train_test_split from scikit-learn\n",
    "    # test_size defines the proportion of the data to use as the test set (default is 20%)\n",
    "    # shuffle=True ensures that the files are shuffled randomly before splitting\n",
    "    train_files, test_files = train_test_split(file_list, test_size=test_size, shuffle=True)\n",
    "\n",
    "    # Return the train and test file lists\n",
    "    return train_files, test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J7-zQgZzP19"
   },
   "source": [
    "## Sliding Window\n",
    "\n",
    "In time series Activity Recognition, a sliding window is a commonly used technique to segment continuous sensor data (such as accelerometer readings) into smaller, fixed-length overlapping or non-overlapping time intervals, or windows. Each window contains a sequence of sensor measurements that represent a short period of time, and this segmented data is used to extract features or make predictions about the activity happening within that window.\n",
    "\n",
    "### Key Concepts of a Sliding Window\n",
    "1.   **Window Size:** This refers to the length of each segment or window, typically defined in terms of the number of time steps or the duration (e.g., 2 seconds). The window size should be chosen carefully to capture enough information about the activity without making the window too large.\n",
    "2.   **Step Size:** The step size determines how far the window moves forward after each step. If the step size is smaller than the window size, the windows will overlap. For example, if the window size is 5 seconds and the step size is 2 seconds, there will be a 3-second overlap between consecutive windows. Overlapping windows provide more data for analysis and can help smooth out predictions by capturing transitional activities.\n",
    "3.   **Non-Overlapping Windows:** If the step size is equal to the window size, the windows do not overlap. This method provides distinct segments of data but may miss transitional phases between activities.\n",
    "\n",
    "### Why Sliding Windows for Activity Recognition?\n",
    "\n",
    "* Segmentation of Continuous Data: Activity recognition systems work with continuous streams of sensor data, and the sliding window helps segment these into manageable pieces to classify activities within specific intervals.\n",
    "\n",
    "* Context Capturing: Human activities are often complex and spread across time. By using a sliding window, you can capture context across a short duration, which may include transitions or small fluctuations in the activity (e.g., a person moving from sitting to standing).\n",
    "\n",
    "* Feature Extraction: Within each window, features such as mean, variance, frequency domain features, etc., can be extracted to help classify the activity.\n",
    "\n",
    "* Real-Time Recognition: In real-time systems, the sliding window allows for continuous monitoring and updating of predictions as new data arrives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "id": "u3SuHww6MpEx"
   },
   "outputs": [],
   "source": [
    "def load_and_apply_sliding_windows(file_paths, window_size, step_size, label):\n",
    "    \"\"\"\n",
    "    Load the data from each file, apply sliding windows, and return the windows and labels.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list): List of file paths to CSV files. Each file contains sensor data (e.g., accelerometer, gyroscope).\n",
    "    window_size (int): The size of each sliding window (number of time steps).\n",
    "    step_size (int): The step size (stride) between consecutive windows.\n",
    "    label (int or str): The label for the activity corresponding to the folder.\n",
    "                        This label will be assigned to each sliding window extracted from the data.\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - windows (numpy.ndarray): A 3D array of sliding windows, where each window has the shape\n",
    "                                   (num_windows, window_size, num_features).\n",
    "        - labels (numpy.ndarray): A 1D array of labels, where each label corresponds to a sliding window.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store sliding windows and their corresponding labels\n",
    "    windows = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each file in the provided file paths\n",
    "    for file_path in file_paths:\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Select the columns containing the necessary sensor data (acceleration and gyroscope readings)\n",
    "        # These columns might vary depending on your dataset's structure\n",
    "        data = data[['accel_x', 'accel_y', 'accel_z']]\n",
    "\n",
    "        # Convert the DataFrame into a numpy array for faster processing in the sliding window operation\n",
    "        data = data.to_numpy()\n",
    "\n",
    "        # Get the number of samples (rows) and features (columns) in the data\n",
    "        num_samples, num_features = data.shape\n",
    "\n",
    "        # Apply sliding windows to the data\n",
    "        # The range function defines the start of each window, moving step_size increments at a time\n",
    "        for i in range(0, num_samples - window_size + 1, step_size):\n",
    "            # Extract a window of size 'window_size' from the current position 'i'\n",
    "            window = data[i:i + window_size, :]\n",
    "\n",
    "            # Append the window to the windows list\n",
    "            windows.append(window)\n",
    "\n",
    "            # Assign the activity label to the window and append it to the labels list\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert the lists of windows and labels into numpy arrays for efficient numerical operations\n",
    "    return np.array(windows), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Ku5P4Lm8QA"
   },
   "source": [
    "## Load and Split Train Test for Each Activity Folder\n",
    "\n",
    "This function processes the sensor data for a specific activity, such as 'walking' or 'running', stored in its respective folder. It splits the data into training and testing sets, applies sliding windows, and labels the windows with the corresponding activity. This function can be used repeatedly for each activity to process and prepare data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "id": "zBVvTBi7N_fh"
   },
   "outputs": [],
   "source": [
    "def process_activity(activity, label, respeck_paths, thingy_paths, window_size=100, step_size=50, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Load data from multiple folders for a specific activity, apply sliding windows, \n",
    "    and split into train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    activity (str): The name of the activity.\n",
    "    label (int): The label associated with the activity.\n",
    "    respeck_paths (list): List of paths for Respeck data folders (e.g., both year1 and year2 folders).\n",
    "    thingy_paths (list): List of paths for Thingy data folders.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Train and test sliding windows and labels for both Respeck and Thingy.\n",
    "    \"\"\"\n",
    "    # Load all files for Respeck and Thingy from specified folders\n",
    "    respeck_files = []\n",
    "    for path in respeck_paths:\n",
    "        respeck_files.extend(load_files_from_multiple_folders(path, [activity]))\n",
    "        \n",
    "    thingy_files = []\n",
    "    for path in thingy_paths:\n",
    "        thingy_files.extend(load_files_from_multiple_folders(path, [activity]))\n",
    "\n",
    "    # Split files into train and test sets\n",
    "    respeck_train_files, respeck_test_files = split_files(respeck_files, test_size)\n",
    "    thingy_train_files, thingy_test_files = split_files(thingy_files, test_size)\n",
    "\n",
    "    # Load and apply sliding windows on Respeck files\n",
    "    respeck_train_windows, respeck_train_labels = load_and_apply_sliding_windows(respeck_train_files, window_size, step_size, label)\n",
    "    respeck_test_windows, respeck_test_labels = load_and_apply_sliding_windows(respeck_test_files, window_size, step_size, label)\n",
    "\n",
    "    # Load and apply sliding windows on Thingy files\n",
    "    thingy_train_windows, thingy_train_labels = load_and_apply_sliding_windows(thingy_train_files, window_size, step_size, label)\n",
    "    thingy_test_windows, thingy_test_labels = load_and_apply_sliding_windows(thingy_test_files, window_size, step_size, label)\n",
    "\n",
    "    return (respeck_train_windows, respeck_train_labels, respeck_test_windows, respeck_test_labels,\n",
    "            thingy_train_windows, thingy_train_labels, thingy_test_windows, thingy_test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv2TUL_Tnnxu"
   },
   "source": [
    "## Combine Data\n",
    "The function combines the sliding window data and their corresponding labels from multiple activities (e.g., walking, running, etc.) into single arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "id": "au6dRWtmmig7"
   },
   "outputs": [],
   "source": [
    "def combine_data(train_test_data, data_type):\n",
    "    \"\"\"\n",
    "    Combines the sliding windows and labels from all activities into a single array for either training or testing.\n",
    "    \n",
    "    Args:\n",
    "        train_test_data (dict): Dictionary containing sliding window data for all activities.\n",
    "        data_type (str): Either 'train' or 'test' to specify which data to combine ('train_windows' or 'test_windows').\n",
    "    \n",
    "    Returns:\n",
    "        tuple: \n",
    "            - respeck_windows (numpy.ndarray): Concatenated Respeck windows.\n",
    "            - respeck_labels (numpy.ndarray): Concatenated Respeck labels.\n",
    "            - thingy_windows (numpy.ndarray): Concatenated Thingy windows.\n",
    "            - thingy_labels (numpy.ndarray): Concatenated Thingy labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract sliding windows and labels for Respeck sensor\n",
    "    respeck_windows_list = [train_test_data[activity][f'respeck_{data_type}_windows'] for activity in train_test_data]\n",
    "    respeck_labels_list = [train_test_data[activity][f'respeck_{data_type}_labels'] for activity in train_test_data]\n",
    "    \n",
    "    # Extract sliding windows and labels for Thingy sensor\n",
    "    thingy_windows_list = [train_test_data[activity][f'thingy_{data_type}_windows'] for activity in train_test_data]\n",
    "    thingy_labels_list = [train_test_data[activity][f'thingy_{data_type}_labels'] for activity in train_test_data]\n",
    "    \n",
    "    # Concatenate windows and labels separately for Respeck\n",
    "    respeck_windows = np.concatenate(respeck_windows_list, axis=0)\n",
    "    respeck_labels = np.concatenate(respeck_labels_list, axis=0)\n",
    "    \n",
    "    # Concatenate windows and labels separately for Thingy\n",
    "    thingy_windows = np.concatenate(thingy_windows_list, axis=0)\n",
    "    thingy_labels = np.concatenate(thingy_labels_list, axis=0)\n",
    "    \n",
    "    return respeck_windows, respeck_labels, thingy_windows, thingy_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Wv1PuOLgUV8"
   },
   "source": [
    "## 1D CNN Model\n",
    "\n",
    "This function, `build_1d_cnn_model`, creates and compiles a 1D Convolutional Neural Network (CNN) for multi-class classification tasks.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "Input Parameters\n",
    "* `input_shape`: Specifies the shape of the input data. It represents (timesteps, features), where timesteps refer to the length of the time series (e.g., 50 windows), and features represent the number of measurements in each time step (e.g., accelerometer readings).\n",
    "* `num_classes`: The number of output classes for the classification problem. For example, if you're classifying six different activities, num_classes would be 6.\n",
    "\n",
    "Returns\n",
    "* The function returns a compiled 1D CNN model that is ready to be trained on your data.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Function Breakdown\n",
    "1.   Model Initialization:\n",
    "    * `model = Sequential()`: Initializes a Sequential model, which means layers will be stacked on top of each other in a linear fashion.\n",
    "2.   First Convolutional Layer\n",
    "    * `Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape)`\n",
    "        * This is the first 1D convolutional layer\n",
    "        * `filters=64`: The layer applies 64 filters (or kernels) over the input data.\n",
    "        * `kernel_size=3`: Each filter will cover 3 timesteps at a time (a window of 3).\n",
    "        * `activation='relu'`: The Rectified Linear Unit (ReLU) activation function introduces non-linearity and helps the model learn complex patterns.\n",
    "        * `input_shape=input_shape`: Specifies the shape of the input data.\n",
    "    * `MaxPooling1D(pool_size=2)`: This pooling layer reduces the dimensionality of the data by taking the maximum value from each 2-timestep window (`pool_size=2`). This helps reduce computational complexity and captures the most important features.\n",
    "3. Second Convolutional Layer:\n",
    "    * `Conv1D(filters=128, kernel_size=3, activation='relu')`\n",
    "        * This is the second convolutional layer, similar to the first, but with 128 filters, which allow the network to learn more complex features from the data.\n",
    "        * `kernel_size=3` and activation='relu' function in the same way as the first Conv1D layer.\n",
    "    * `MaxPooling1D(pool_size=2)`: Another pooling layer to downsample the output, further reducing the data’s dimensionality.\n",
    "4. Flattening Layer:\n",
    "    * `Flattening`: Converts the 2D output of the convolutional and pooling layers into a 1D vector. This is necessary because the next layer is fully connected, and it requires a 1D input.\n",
    "5. Fully Connected Layer:\n",
    "    * `Dense(128, activation='relu')`: This is a fully connected layer with 128 units/neurons. Each neuron is connected to every input from the flattened output. The ReLU activation function is used again to introduce non-linearity and help the model learn complex relationships.\n",
    "6. Dropout Layer:\n",
    "    * `Dropout(0.5)`: This layer randomly sets 50% of the neurons to zero during training to prevent overfitting. It helps the model generalize better to unseen data.\n",
    "7. Output Layer:\n",
    "    * `Dense(num_classes, activation='softmax')`: This is the output layer with num_classes neurons, one for each class in the classification problem. The softmax activation function ensures the output values represent probabilities that sum to 1, useful for multi-class classification.\n",
    "8. Compiling the model\n",
    "    * model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):\n",
    "        * Optimizer: 'adam': Adam is an optimization algorithm that adjusts the learning rate during training to improve performance.\n",
    "        * Loss: 'categorical_crossentropy': This loss function is used for multi-class classification problems where the target variable is one-hot encoded (i.e., represented as a vector of 0s and 1s).\n",
    "        * Metrics: ['accuracy']: The accuracy metric is used to evaluate the model’s performance during training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "id": "sCOkh99EOg8t"
   },
   "outputs": [],
   "source": [
    "def build_1d_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds and compiles a 1D CNN model for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (timesteps, features).\n",
    "        num_classes (int): The number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        model (Sequential): Compiled 1D CNN model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Conv1D layer\n",
    "    # You can try experimenting with different filters, kernel_size values and activiation functions\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    # You can try experimenting with different filters, kernel_size values and activiation functions\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Add LSTM layer\n",
    "    model.add(LSTM(64, return_sequences=True))  # Use return_sequences=True if adding more layers after LSTM\n",
    "\n",
    "    # Flatten the output from the convolutional layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Dropout layer for regularization\n",
    "    # You can try experimenting with different dropout rates\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer with softmax for multi-class classification\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #  Prints a detailed summary of the model, showing the layers, their output shapes, and the number of trainable parameters\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "# from tensorflow.keras.layers import Dense, Dropout, concatenate\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# def build_dual_input_cnn_model(respeck_input_shape, thingy_input_shape, num_classes):\n",
    "#     respeck_input = Input(shape=respeck_input_shape, name='respeck_input')\n",
    "#     thingy_input = Input(shape=thingy_input_shape, name='thingy_input')\n",
    "    \n",
    "#     # Respeck branch with BatchNorm and Global Average Pooling\n",
    "#     x_respeck = Conv1D(64, 3, activation='relu')(respeck_input)\n",
    "#     x_respeck = MaxPooling1D(2)(x_respeck)\n",
    "#     x_respeck = Conv1D(128, 3, activation='relu')(x_respeck)\n",
    "#     x_respeck = MaxPooling1D(2)(x_respeck)\n",
    "#     x_respeck = GlobalAveragePooling1D()(x_respeck)\n",
    "\n",
    "#     # Thingy branch with BatchNorm and Global Average Pooling\n",
    "#     x_thingy = Conv1D(64, 3, activation='relu')(thingy_input)\n",
    "#     x_thingy = MaxPooling1D(2)(x_thingy)\n",
    "#     x_thingy = Conv1D(128, 3, activation='relu')(x_thingy)\n",
    "#     x_thingy = MaxPooling1D(2)(x_thingy)\n",
    "#     x_thingy = GlobalAveragePooling1D()(x_thingy)\n",
    "\n",
    "#     # Combine branches\n",
    "#     combined_output = concatenate([x_respeck, x_thingy])\n",
    "    \n",
    "#     # Dense layers with L2 regularization and Dropout\n",
    "#     combined_output = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined_output)  \n",
    "#     combined_output = Dropout(0.5)(combined_output)\n",
    "\n",
    "#     # Output layer with softmax for multi-class classification\n",
    "#     output_layer = Dense(num_classes, activation='softmax')(combined_output)\n",
    "\n",
    "#     # Compile model\n",
    "#     model = Model(inputs=[respeck_input, thingy_input], outputs=output_layer)\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=Adam(),\n",
    "#         loss='categorical_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "def build_dual_input_cnn_model(respeck_input_shape, thingy_input_shape, num_classes):\n",
    "    # Respeck branch\n",
    "    respeck_input = Input(shape=respeck_input_shape, name='respeck_input')\n",
    "    x_respeck = Conv1D(128, 5, activation='relu', padding='same')(respeck_input)\n",
    "    x_respeck = BatchNormalization()(x_respeck)\n",
    "    x_respeck = MaxPooling1D(2)(x_respeck)\n",
    "    x_respeck = Dropout(0.3)(x_respeck)\n",
    "\n",
    "    x_respeck = Conv1D(256, 3, activation='relu', padding='same')(x_respeck)\n",
    "    x_respeck = BatchNormalization()(x_respeck)\n",
    "    x_respeck = MaxPooling1D(2)(x_respeck)\n",
    "    x_respeck = Dropout(0.3)(x_respeck)\n",
    "\n",
    "    x_respeck = GlobalAveragePooling1D()(x_respeck)\n",
    "\n",
    "    # Thingy branch\n",
    "    thingy_input = Input(shape=thingy_input_shape, name='thingy_input')\n",
    "    x_thingy = Conv1D(128, 5, activation='relu', padding='same')(thingy_input)\n",
    "    x_thingy = BatchNormalization()(x_thingy)\n",
    "    x_thingy = MaxPooling1D(2)(x_thingy)\n",
    "    x_thingy = Dropout(0.3)(x_thingy)\n",
    "\n",
    "    x_thingy = Conv1D(256, 3, activation='relu', padding='same')(x_thingy)\n",
    "    x_thingy = BatchNormalization()(x_thingy)\n",
    "    x_thingy = MaxPooling1D(2)(x_thingy)\n",
    "    x_thingy = Dropout(0.3)(x_thingy)\n",
    "\n",
    "    x_thingy = GlobalAveragePooling1D()(x_thingy)\n",
    "\n",
    "    # Concatenate branches directly without attention\n",
    "    combined = concatenate([x_respeck, x_thingy])\n",
    "\n",
    "    # Fully connected layer with dropout and L2 regularization\n",
    "    combined = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
    "    combined = Dropout(0.5)(combined)\n",
    "\n",
    "    # Output layer for multi-class classification\n",
    "    output_layer = Dense(num_classes, activation='softmax')(combined)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[respeck_input, thingy_input], outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HurfE6lmOjQT"
   },
   "source": [
    "# Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLs1eacYoa_S"
   },
   "source": [
    "## Step 1: Prepare and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "id": "QTzHon1EoiEK"
   },
   "outputs": [],
   "source": [
    "# Define activity folders and corresponding labels\n",
    "# Each key is the name of the physical activity, and the corresponding value is the numeric label\n",
    "# These labels will be used as the target variable for classification.\n",
    "activities = {\n",
    "    'sittingStanding': 0,\n",
    "    'lyingBack': 1,\n",
    "    'lyingLeft': 2,\n",
    "    'lyingRight': 3,\n",
    "    'lyingStomach': 4,\n",
    "    'miscMovement': 5,\n",
    "    'normalWalking': 6,\n",
    "    'running': 7,\n",
    "    'shuffleWalking': 8,\n",
    "    'ascending': 9,\n",
    "    'descending': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "id": "2FqqaaX2o1YW"
   },
   "outputs": [],
   "source": [
    "# Dictionary to store sliding windows and labels for both train and test sets for each activity\n",
    "train_test_data = {}\n",
    "\n",
    "# Loop through each activity folder and process the data\n",
    "for activity, label in activities.items():\n",
    "    train_test_data[activity] = {}\n",
    "\n",
    "    # Call process_activity() with respeck_paths and thingy_paths\n",
    "    (train_test_data[activity]['respeck_train_windows'], train_test_data[activity]['respeck_train_labels'],\n",
    "     train_test_data[activity]['respeck_test_windows'], train_test_data[activity]['respeck_test_labels'],\n",
    "     train_test_data[activity]['thingy_train_windows'], train_test_data[activity]['thingy_train_labels'],\n",
    "     train_test_data[activity]['thingy_test_windows'], train_test_data[activity]['thingy_test_labels']) = process_activity(\n",
    "        activity, label, respeck_paths, thingy_paths)\n",
    "\n",
    "# Explanation:\n",
    "# - `respeck_paths` and `thingy_paths` now contain both year1 and year2 folders for each dataset type.\n",
    "# - `process_activity` will handle loading, splitting, and processing data across all specified folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdGR352hph4X"
   },
   "source": [
    "Now that each activity has been processed and stored in train_test_data, we need to combine the sliding windows and labels from all activities into unified arrays (one for training and one for testing) for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "id": "OtpVBr4Fpq_8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_respeck shape: (16432, 100, 3), y_train_respeck shape: (16432,)\n",
      "X_train_thingy shape: (17428, 100, 3), y_train_thingy shape: (17428,)\n",
      "X_test_respeck shape: (4197, 100, 3), y_test_respeck shape: (4197,)\n",
      "X_test_thingy shape: (4442, 100, 3), y_test_thingy shape: (4442, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_respeck, y_train_respeck, X_train_thingy, y_train_thingy = combine_data(train_test_data, 'train')\n",
    "X_test_respeck, y_test_respeck, X_test_thingy, y_test_thingy = combine_data(train_test_data, 'test')\n",
    "\n",
    "# Print shapes to verify correctness\n",
    "print(f\"X_train_respeck shape: {X_train_respeck.shape}, y_train_respeck shape: {y_train_respeck.shape}\")\n",
    "print(f\"X_train_thingy shape: {X_train_thingy.shape}, y_train_thingy shape: {y_train_thingy.shape}\")\n",
    "print(f\"X_test_respeck shape: {X_test_respeck.shape}, y_test_respeck shape: {y_test_respeck.shape}\")\n",
    "print(f\"X_test_thingy shape: {X_test_thingy.shape}, y_test_thingy shape: {X_test_thingy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_respeck shape: (16432, 100, 3), y_train_respeck shape: (16432,)\n",
      "X_train_thingy shape: (17428, 100, 3), y_train_thingy shape: (17428,)\n",
      "X_test_respeck shape: (4197, 100, 3), y_test_respeck shape: (4197,)\n",
      "X_test_thingy shape: (4442, 100, 3), y_test_thingy shape: (4442,)\n"
     ]
    }
   ],
   "source": [
    "# No need to equalize samples anymore since we want to keep all windows as they are\n",
    "\n",
    "# Print shapes of training data to verify correctness\n",
    "print(f\"X_train_respeck shape: {X_train_respeck.shape}, y_train_respeck shape: {y_train_respeck.shape}\")\n",
    "print(f\"X_train_thingy shape: {X_train_thingy.shape}, y_train_thingy shape: {y_train_thingy.shape}\")\n",
    "\n",
    "# Print shapes of testing data to verify correctness\n",
    "print(f\"X_test_respeck shape: {X_test_respeck.shape}, y_test_respeck shape: {y_test_respeck.shape}\")\n",
    "print(f\"X_test_thingy shape: {X_test_thingy.shape}, y_test_thingy shape: {y_test_thingy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yQGU1vwIQdz"
   },
   "source": [
    "### One-Hot Encode Labels (for multi-class classification)\n",
    "If you have more than two classes, you'll need to one-hot encode the labels, especially if your model will use categorical cross-entropy loss.\n",
    "\n",
    "One-Hot Encoding converts categorical labels into binary vectors (one-hot encoded format). Each class label is represented as a binary vector with 1 for the correct class and 0 for others. This is necessary for training models that use categorical_crossentropy as the loss function, such as a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_respeck_one_hot shape: (16432, 11), y_test_respeck_one_hot shape: (4197, 11)\n",
      "y_train_thingy_one_hot shape: (17428, 11), y_test_thingy_one_hot shape: (4442, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Initialize separate OneHotEncoders for Respeck and Thingy\n",
    "encoder_respeck = OneHotEncoder(sparse_output=False)\n",
    "encoder_thingy = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit encoders on training data only (to avoid data leakage)\n",
    "encoder_respeck.fit(y_train_respeck.reshape(-1, 1))\n",
    "encoder_thingy.fit(y_train_thingy.reshape(-1, 1))\n",
    "\n",
    "# One-hot encode Respeck labels (train and test)\n",
    "y_train_respeck_one_hot = encoder_respeck.transform(y_train_respeck.reshape(-1, 1))\n",
    "y_test_respeck_one_hot = encoder_respeck.transform(y_test_respeck.reshape(-1, 1))\n",
    "\n",
    "# One-hot encode Thingy labels (train and test)\n",
    "y_train_thingy_one_hot = encoder_thingy.transform(y_train_thingy.reshape(-1, 1))\n",
    "y_test_thingy_one_hot = encoder_thingy.transform(y_test_thingy.reshape(-1, 1))\n",
    "\n",
    "# Print shapes of one-hot encoded labels to verify correctness\n",
    "print(f\"y_train_respeck_one_hot shape: {y_train_respeck_one_hot.shape}, y_test_respeck_one_hot shape: {y_test_respeck_one_hot.shape}\")\n",
    "print(f\"y_train_thingy_one_hot shape: {y_train_thingy_one_hot.shape}, y_test_thingy_one_hot shape: {y_test_thingy_one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_respeck shape: (16432, 100, 3)\n",
      "X_train_thingy shape: (17428, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Verify Respeck input shape\n",
    "print(f\"X_train_respeck shape: {X_train_respeck.shape}\")  # Should be (num_samples, window_size, num_features)\n",
    "\n",
    "# Verify Thingy input shape\n",
    "print(f\"X_train_thingy shape: {X_train_thingy.shape}\")  # Should be (num_samples, window_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Find the maximum sample size between Respeck and Thingy\n",
    "max_samples = max(X_train_respeck.shape[0], X_train_thingy.shape[0])\n",
    "\n",
    "# Resample Respeck data to match Thingy's sample size (upsampling)\n",
    "X_train_respeck, y_train_respeck_one_hot = resample(\n",
    "    X_train_respeck, y_train_respeck_one_hot,\n",
    "    replace=True,  # Upsampling with replacement\n",
    "    n_samples=max_samples,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# No need to resample Thingy if it's already larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_respeck shape: (17428, 100, 3)\n",
      "X_train_thingy shape: (17428, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Verify Respeck input shape\n",
    "print(f\"X_train_respeck shape: {X_train_respeck.shape}\")  # Should be (num_samples, window_size, num_features)\n",
    "\n",
    "# Verify Thingy input shape\n",
    "print(f\"X_train_thingy shape: {X_train_thingy.shape}\")  # Should be (num_samples, window_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (Respeck): 11\n",
      "Number of classes (Thingy): 11\n"
     ]
    }
   ],
   "source": [
    "# Verify number of output classes\n",
    "print(f\"Number of classes (Respeck): {y_train_respeck_one_hot.shape[1]}\")\n",
    "print(f\"Number of classes (Thingy): {y_train_thingy_one_hot.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Resample Respeck test data to match Thingy's sample size\n",
    "X_test_respeck, y_test_respeck_one_hot = resample(\n",
    "    X_test_respeck, y_test_respeck_one_hot,\n",
    "    replace=True,\n",
    "    n_samples=X_test_thingy.shape[0],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_dict = {\n",
    "    0: 0.8,  # Well-performing class\n",
    "    1: 0.7,  # Well-performing class\n",
    "    2: 1.5,  # Slightly underperforming\n",
    "    3: 1.0,  \n",
    "    4: 0.8,  # Well-performing class\n",
    "    5: 2.5,  # Underperforming class (F1 ~65%)\n",
    "    6: 4.0,  # Underperforming class (F1 ~87%)\n",
    "    7: 1.3,\n",
    "    8: 4.0,  # Underperforming class (F1 ~75%)\n",
    "    9: 2.5,\n",
    "    10: 1.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEhUxZzzJzzI"
   },
   "source": [
    "## Step 2: Build the 1D-CNN Model\n",
    "Call our `build_1d_cnn_model` functionto build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "4sDZWZH_KKBD",
    "outputId": "12cc6048-0921-414c-d2d7-b8d5268a8196"
   },
   "outputs": [],
   "source": [
    "# Determine the input shape for the model\n",
    "respeck_input_shape = (X_train_respeck.shape[1], X_train_respeck.shape[2])\n",
    "thingy_input_shape = (X_train_thingy.shape[1], X_train_thingy.shape[2])\n",
    "\n",
    "# Determine the number of output classes\n",
    "num_classes = y_train_respeck_one_hot.shape[1]  # This should be the same for both sensors\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_dual_input_cnn_model(respeck_input_shape, thingy_input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_respeck shape: (4442, 100, 3)\n",
      "X_test_thingy shape: (4442, 100, 3)\n",
      "y_test_respeck_one_hot shape: (4442, 11)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_test_respeck shape: {X_test_respeck.shape}\")\n",
    "print(f\"X_test_thingy shape: {X_test_thingy.shape}\")\n",
    "print(f\"y_test_respeck_one_hot shape: {y_test_respeck_one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1-SHEmtKM0D"
   },
   "source": [
    "## Step 3: Train the CNN Model\n",
    "\n",
    "Train the 1D CNN model using the training data and validate on the test data. The model will learn to map input sliding windows to their corresponding activity labels.\n",
    "\n",
    "`model.fit()` is used to train the neural network model. It takes several parameters:\n",
    "* `X_train`: The input training data (sliding windows), with shape (num_samples, window_size, num_features).\n",
    "* `y_train_one_hot`: The corresponding one-hot encoded labels for the training data, with shape (num_samples, num_classes).\n",
    "* `epochs`: Number of times the entire training dataset is passed through the model. You can try adjusting the number of epochs and compare the difference in model performance. In this case, we are training for 20 epochs, meaning the model will see the entire training set 20 times.\n",
    "* `batch_size`: Number of samples processed before the model's weights are updated. Here, the batch size is set to 32, meaning the model will process 32 samples at a time before updating its parameters.\n",
    "* `validation_data`: This parameter allows us to evaluate the model's performance on the test data after each epoch.\n",
    "*`(X_test, y_test_one_hot)`: These are the input test data and corresponding one-hot encoded test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4zICRasKPsT",
    "outputId": "c2c4bde2-1417-4c96-e25b-8e4f3d039bf9"
   },
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     [X_train_respeck, X_train_thingy],  # Two inputs: Respeck and Thingy features\n",
    "#     y_train_respeck_one_hot,\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "#     validation_data=([X_test_respeck, X_test_thingy], y_test_respeck_one_hot)  # Validation inputs and labels\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['respeck_input', 'thingy_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 22ms/step - accuracy: 0.5739 - loss: 3.0956 - val_accuracy: 0.6824 - val_loss: 1.4673\n",
      "Epoch 2/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - accuracy: 0.7983 - loss: 1.6007 - val_accuracy: 0.8222 - val_loss: 0.9781\n",
      "Epoch 3/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - accuracy: 0.8664 - loss: 1.1497 - val_accuracy: 0.8593 - val_loss: 0.9181\n",
      "Epoch 4/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.8942 - loss: 0.9369 - val_accuracy: 0.8307 - val_loss: 1.0663\n",
      "Epoch 5/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.9021 - loss: 0.8578 - val_accuracy: 0.8424 - val_loss: 1.0074\n",
      "Epoch 6/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.9207 - loss: 0.7210 - val_accuracy: 0.8352 - val_loss: 0.9623\n",
      "Epoch 7/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.9212 - loss: 0.6811 - val_accuracy: 0.8834 - val_loss: 0.8137\n",
      "Epoch 8/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 25ms/step - accuracy: 0.9232 - loss: 0.6460 - val_accuracy: 0.7830 - val_loss: 1.1202\n",
      "Epoch 9/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 26ms/step - accuracy: 0.9365 - loss: 0.5341 - val_accuracy: 0.8823 - val_loss: 0.8088\n",
      "Epoch 10/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 26ms/step - accuracy: 0.9407 - loss: 0.4905 - val_accuracy: 0.8800 - val_loss: 0.7491\n",
      "Epoch 11/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 26ms/step - accuracy: 0.9467 - loss: 0.4621 - val_accuracy: 0.8739 - val_loss: 0.8221\n",
      "Epoch 12/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 27ms/step - accuracy: 0.9451 - loss: 0.4532 - val_accuracy: 0.8780 - val_loss: 0.7088\n",
      "Epoch 13/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 28ms/step - accuracy: 0.9460 - loss: 0.4364 - val_accuracy: 0.8852 - val_loss: 0.7077\n",
      "Epoch 14/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 27ms/step - accuracy: 0.9520 - loss: 0.3960 - val_accuracy: 0.8836 - val_loss: 0.7710\n",
      "Epoch 15/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 27ms/step - accuracy: 0.9531 - loss: 0.3843 - val_accuracy: 0.8845 - val_loss: 0.8057\n",
      "Epoch 16/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 29ms/step - accuracy: 0.9593 - loss: 0.3662 - val_accuracy: 0.8879 - val_loss: 0.7877\n",
      "Epoch 17/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 28ms/step - accuracy: 0.9555 - loss: 0.3559 - val_accuracy: 0.8715 - val_loss: 0.7574\n",
      "Epoch 18/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 29ms/step - accuracy: 0.9598 - loss: 0.3512 - val_accuracy: 0.8852 - val_loss: 0.7886\n",
      "Epoch 19/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 29ms/step - accuracy: 0.9585 - loss: 0.3359 - val_accuracy: 0.8913 - val_loss: 0.7696\n",
      "Epoch 20/20\n",
      "\u001b[1m545/545\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 31ms/step - accuracy: 0.9593 - loss: 0.3324 - val_accuracy: 0.8935 - val_loss: 0.7139\n"
     ]
    }
   ],
   "source": [
    "# Train the model with class weights\n",
    "history = model.fit(\n",
    "    [X_train_respeck, X_train_thingy],   # Two inputs: Respeck and Thingy features\n",
    "    y_train_respeck_one_hot,             # One-hot encoded labels\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_test_respeck, X_test_thingy], y_test_respeck_one_hot),  # Validation inputs and labels\n",
    "    class_weight=class_weights_dict      # Pass computed class weights here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrSPBPh4KcBn"
   },
   "source": [
    "## Step 4: Evaluate the Model\n",
    "After training, you can evaluate the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UB2Bi7ieKelv",
    "outputId": "de615d87-81e5-45df-f50b-60b21139fdd0"
   },
   "outputs": [],
   "source": [
    "# # Get predicted probabilities for the test set\n",
    "# y_pred_probs = model.predict([X_test_respeck, X_test_thingy])\n",
    "\n",
    "# # Convert the predicted probabilities to class labels\n",
    "# y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# # Convert the true test labels from one-hot encoding back to class labels\n",
    "# y_true_classes_respeck = np.argmax(y_test_respeck_one_hot, axis=1)\n",
    "# y_true_classes_thingy = np.argmax(y_test_thingy_one_hot, axis=1)\n",
    "\n",
    "# # Combine the true classes from both sensors\n",
    "# y_true_classes = np.concatenate([y_true_classes_respeck, y_true_classes_thingy])\n",
    "\n",
    "# # Generate the classification report\n",
    "# report = classification_report(y_true_classes, y_pred_classes, digits=4)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9361    0.9455    0.9408       697\n",
      "           1     1.0000    0.9924    0.9962       395\n",
      "           2     0.8225    0.9104    0.8642       346\n",
      "           3     1.0000    0.8349    0.9100       327\n",
      "           4     0.9017    1.0000    0.9483       413\n",
      "           5     0.9015    0.8444    0.8720       347\n",
      "           6     0.8244    0.8934    0.8575       394\n",
      "           7     0.9225    0.9835    0.9520       363\n",
      "           8     0.7210    0.8005    0.7587       381\n",
      "           9     0.9262    0.8289    0.8748       409\n",
      "          10     0.8914    0.7324    0.8042       370\n",
      "\n",
      "    accuracy                         0.8935      4442\n",
      "   macro avg     0.8952    0.8878    0.8890      4442\n",
      "weighted avg     0.8976    0.8935    0.8933      4442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model.predict([X_test_respeck, X_test_thingy])\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert the true test labels from one-hot encoding back to class labels\n",
    "# Use only one set of true labels (e.g., Respeck)\n",
    "y_true_classes = np.argmax(y_test_respeck_one_hot, axis=1)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true_classes, y_pred_classes, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QXKh8ritRbw"
   },
   "source": [
    "As you can see from the model performance results, the classification performance isn't exactly impressive. For Coursework 3, your group should explore and experiment with various models, parameters, and techniques in order to improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHNzGOHDtnQ4"
   },
   "source": [
    "# Exporting your model to TFLite\n",
    "\n",
    "You can use the TFLiteConverter class provided by TensorFlow to convert your trained model into the TensorFlow Lite format. We export models to TensorFlow Lite (TFLite) for several reasons, primarily because TFLite is designed for deployment on edge devices, such as mobile phones, embedded systems, IoT devices, and microcontrollers, where computational resources and power are limited. This is necessary as you will be running your ML models on your Android devices to perform live classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_QKoJtmufDa",
    "outputId": "48d281ab-4ed3-4ee1-fd47-30f7f4c10b96"
   },
   "outputs": [],
   "source": [
    "# # Convert the trained Keras model to TensorFlow Lite format\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)  # model is your trained Keras model\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the converted model to a .tflite file\n",
    "# with open('model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "\n",
    "# print(\"Model successfully exported to model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nikit\\AppData\\Local\\Temp\\tmpzq2w_upw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nikit\\AppData\\Local\\Temp\\tmpzq2w_upw\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\nikit\\AppData\\Local\\Temp\\tmpzq2w_upw'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 100, 3), dtype=tf.float32, name='respeck_input'), TensorSpec(shape=(None, 100, 3), dtype=tf.float32, name='thingy_input')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 11), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  3193277457584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277716032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277454768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277463392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277716384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277713568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277712336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277714272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277455120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277458464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277463920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277457232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277725008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277721840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277456880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277666176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277725536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277719728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277714800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277723776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277667408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277667936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277668992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277664944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277897312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277894320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277901536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  3193277903120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model successfully exported to C:/Users/nikit/Documents/University/Year4/PDIOT/CW3/models/activities_model.tflite\n"
     ]
    }
   ],
   "source": [
    "# Convert the trained Keras model to TensorFlow Lite format\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)  # 'model' is your trained Keras model\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Specify the path where the .tflite model should be saved\n",
    "    tflite_model_path = 'C:/Users/nikit/Documents/University/Year4/PDIOT/CW3/models/activities_model.tflite'\n",
    "\n",
    "    # Save the converted model to the specified .tflite file\n",
    "    with open(tflite_model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"Model successfully exported to {tflite_model_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model export: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to look into the average accuraces of static and dynamic activities separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGOj682D0PfS"
   },
   "source": [
    "# Good job!\n",
    "This is the end of Lab 3. In the next lab, you will focus on deploying your machine learning model onto your Android App in order to classify activities in real-time."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
